# Discrete vs continuous cases

We distinguish between _discrete_ and _continuous_ problems: in the discrete
scenario, our sample space is countable; in the continuous it is uncountable.
The distinction is useful because in the discrete case, we can _always_ use the
discrete $\sigma$-algebra on the sample space as our event space.
In the continuous case this isn't possible in general, because there are _too
many_ events in the discrete $\sigma$-algebra to measure.

We'll also show that a _probability mass function_ can be used to define a
probability measure in the discrete case, and likewise with _probability density
functions_ in the continuous case.

## Discrete

If the sample space $\Omega$ is countable, then we can define a probability
measure by simply giving the probability for each outcome in $\Omega$.
$\sigma$-additivity implies that the probability of any event in the event space
is just the (countable) sum of the probabilities of the outcomes.
That is, if we know $\{ p_\omega \mid \omega \in \Omega \}$, then $P(E) = \sum_{\omega \in E} p_\omega$.
The function $p : \Omega \to [0, 1]$ is called a _probability mass function_.
Naturally, we must also have $\sum_{\omega \in \Omega} p_\omega = 1$.

A few questions come to mind: do we need to worry about convergence when
computing this sum?
For instance, is it possible that $\sum_{\omega \in E} p_\omega$ has different
values depending on how we order the elements in the event $E$?
No, since $p$ (as a sequence) converges _absolutely_[^absolute_convergence].

[^absolute_convergence]: Specifically, if a series converges absolutely, then a
  series constructed from any reordering of the sequence of terms also converges
  to the same value.

It's worth noting that in this case we can measure (i.e. define a probability
for) _any_ subset of the sample space.
That is, our event space is the discrete $\sigma$-algebra over the sample space.

## Continuous

If our sample space is uncountable we can't always define the probability of an
event as a sum of the probabilities of the outcomes in it, since the event may
be uncountable.
We can still define a function $f : \Omega \to [0, 1]$ such that $\int_\Omega f = 1$.
In order for this integral to make sense, $f$ must be _measurable_.
In this case, we call $f$ a _probability density function_.

The probability of any event $E$ is then integral

$$
\int_E f(x) \,dx
$$

## The importance of counting

The beginning of most introductions to probability are filled with counting
problems.
Why is combinatorics so important in the discrete case?
Are all discrete problems reducible to counting problems?

As we've seen, we sidestep the tricky question of _how_ nature randomly picks
outcomes in probability by encoding the "likelihood" in the probability measure.
Combinatorics is important in the discrete case because determining the
likelihood of events is often reducible to counting.
As a toy example, suppose we have an urn containing 3 red balls, 2 blue balls,
and 5 yellow balls.
If we draw two balls without replacing the first, the probability of picking two
reds is simply the number of ways of picking two reds divided by the total
number of ways of picking two balls:

$$
P(\{ R, R \}) = \frac{3 \cdot 2}{10 \cdot 9} = \frac{1}{15}
$$

What if we replace the first ball drawn?
Then

$$
P(\{ R, R \}) = \frac{3 \cdot 3}{10 \cdot 10} = \frac{9}{100}
$$

"Simulating" these experiments in JavaScript confirms these calculations.

## Examples
