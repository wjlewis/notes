# Linear maps

## Linear maps are special

I'm thinking of a function $f : \mathbb{R}^2 \to \mathbb{R}^2$.
There are a _lot_ of functions from $\mathbb{R}^2$ to $\mathbb{R}^2$, so I'll
give you a hint: $f(1, 0) = (2, -1)$ and $f(0, 1) = (5, 3)$.
_Any_ hint is better than no hint, but it still leaves an uncountable ocean of
possibilities.
Even if I tell you that $f$ is continuous, or differentiable, or
twice-differentiable, or smooth, or a polynomial, you'll remain very much in the
dark.
But if I tell you that $f$ is _linear_, you know everything there is to know
about it.

## Linear maps

_We'll just focus on $\mathbb{R}^n$ in this note, although most of what we'll say
applies to any finite-dimensional vector space._

A function $L : \mathbb{R}^m \to \mathbb{R}^n$ is _linear_ iff

1. $L(x + y) = L(x) + L(y)$ for all $x, y \in \mathbb{R}^m$
2. $L(\lambda x) = \lambda L(x)$ for all $x \in \mathbb{R}^m$ and $\lambda \in \mathbb{R}$.

An immediate, but perhaps not obvious consequence of these properties is this:

_$L$'s behavior is entirely determined by how it transforms a basis of
$\mathbb{R}^m$._

This is the special quality alluded to in the introduction: once you know what
$L$ does to a basis of its domain, you know how it transforms _every_ point in
the domain.

_Proof_: let $w \in \mathbb{R}^m$ be any point in $L$'s domain, let $u_1, \ldots, u_m \in \mathbb{R}^m$
be a basis for $\mathbb{R}^m$, and $v_1, \ldots, v_n \in \mathbb{R}^n$ be a basis
for $\mathbb{R}^n$.
Because $u_1, \ldots, u_m$ form a basis for the domain, $w$ can be expressed as
a linear combination of them:

$$
w = c_1 u_1 + c_2 u_2 + \cdots + c_m u_m
$$

Then

$$
\begin{align*}
L(w) &= L(c_1 u_1 + c_2 u_2 + \cdots + c_m u_m) \\
     &= L(c_1 u_1) + L(c_2 u_2) + \cdots + L(c_m u_m) \\
     &= c_1 L(u_1) + c_2 L(u_2) + \cdots + c_m L(u_m)
\end{align*}
$$

This is just another way of saying that $L(w)$ is a linear combination of the
images of the basis vectors (and this linear combination has the exact same
coefficients).

## Matrices

Since each $L(u_k)$ lives in $\mathbb{R}^n$, we can represent it as a linear
combination of $v_1, \ldots, v_n$.
That is,

$$
\begin{align*}
L(u_1) &= a_{11} v_1 + a_{21} v_2 + \cdots + a_{n1} v_n \\
L(u_2) &= a_{12} v_1 + a_{22} v_2 + \cdots + a_{n2} v_n \\
\vdots \\
L(u_m) &= a_{1m} v_1 + a_{2m} v_2 + \cdots + a_{nm} v_n
\end{align*}
$$

So

$$
\begin{align*}
L(w) &= c_1 L(u_1) + \cdots + c_m L(u_m) \\
     &= c_1 (a_{11} v_1 + \cdots a_{n1} v_n) + c_2 (a_{12} v_1 + \cdots a_{n2} v_n) + \cdots + c_m (a_{1m} v_1 + \cdots a_{nm} v_n) \\
     &= (a_{11} c_1 + a_{12} c_2 + \cdots + a_{1m} c_m) v_1 + \cdots + (a_{n1} c_1 + a_{n2} c_2 + \cdots + a_{nm} c_m) v_n \\
     &=
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1m} \\
a_{21} & a_{22} & \cdots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nm}
\end{bmatrix}
\begin{bmatrix}
c_1 \\
c_2 \\
\vdots \\
c_m
\end{bmatrix}
\end{align*}
$$

So a matrix is nothing more than a compact representation of a linear transform.
Once a basis for the domain and a basis for the codomain have been selected,
every linear transform corresponds to a matrix, and every matrix to a linear
transform.

Applying the linear transform to a vector $w$ is equivalent to multiplying the
matrix times the representation of $w$ in terms of the matrix's "domain basis".

It's also clear that the columns of the matrix are the images of the domain
basis under $L$.
